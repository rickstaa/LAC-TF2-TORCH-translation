"""LAC algorithm class

This module contains a Tensorflow 2.x implementation of the Lyapunov Actor Critic (LAC)
Reinforcement learning algorithm of
[Han et al. 2019](https://arxiv.org/pdf/2004.14288.pdf). It also contains the
Soft Actor critic of [Haarnoja et al. 2019](https://arxiv.org/abs/1812.05905). One can
switch between the two algorithms by setting the `use_lyapunov` variable in the
variant.py file.

.. note::
    Code Conventions:
        - We use a `_` suffix to distinguish the target network from the main
            network.
        - We use a2 to specify an action that was generated by the Gaussian actor.
        - When in the comments an equation (eq.) is specified without a prefix this
            regards an equation in Han et. al 2019.
"""

import time
from collections import deque
import os
import sys
import random
import os.path as osp
import json

import numpy as np
import tensorflow as tf

from gaussian_actor import SquashedGaussianActor
from lyapunov_critic import LyapunovCritic
from q_critic import QCritic
from utils import (
    get_env_from_name,
    colorize,
    evaluate_training_rollouts,
    training_evaluation,
)
from serialization_utils import convert_json
from pool import Pool
import logger

# Script settings
from variant import (
    USE_GPU,
    ENV_NAME,
    RANDOM_SEED,
    ENV_SEED,
    TRAIN_PARAMS,
    ALG_PARAMS,
    ENVS_PARAMS,
    SCALE_LAMBDA_MIN_MAX,
    SCALE_ALPHA_MIN_MAX,
    DEBUG_PARAMS,
)

# Set random seed to get comparable results for each run
if RANDOM_SEED is not None:
    os.environ["PYTHONHASHSEED"] = str(RANDOM_SEED)
    os.environ["TF_CUDNN_DETERMINISTIC"] = "1"  # new flag present in tf 2.0+
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    tf.random.set_seed(RANDOM_SEED)

# Disable GPU if requested
if not USE_GPU:
    tf.config.set_visible_devices([], "GPU")
    print("Tensorflow is using CPU")
else:
    print("Tensorflow is using GPU")

# Disable tf.function graph execution if debug
if DEBUG_PARAMS["debug"]:
    print(
        colorize(
            (
                "WARN: tf.functions are executed in eager mode because DEBUG=True. "
                "This significantly slow down the training. Please disable DEBUG "
                "during deployment."
            ),
            "yellow",
            bold=True,
        )
    )
    tf.config.run_functions_eagerly(True)


class LAC(tf.Module):
    """The Lyapunov actor critic.

    Attributes:
        ga (tf.keras.Model): The Squashed Gaussian Actor network.

        ga_ (tf.keras.Model): The Squashed Gaussian Actor target network.

        lc (tf.keras.Model: The Lyapunov Critic network.

        lc_ (tf.keras.Model: The Lyapunov Critic target network.

        q_1 (tf.keras.Model: The first Q-Critic network.

        q_2 (tf.keras.Model: The second Q-Critic network.

        q_2_ (tf.keras.Model: The first Q-Critic target network.

        q_2_ (tf.keras.Model): The second Q-Crictic target network.

        log_alpha (tf.Variable): The temperature lagrance multiplier.

        log_labda (tf.Variable): The Lyapunov lagrance multiplier.

        target_entropy (int): The target entropy.

        device (str): The device the networks are placed on (CPU or GPU).

        use_lyapunov (bool): Whether the Lyapunov Critic is used (use_lyapunov=True) or
            the regular Q-critic (use_lyapunov=false).
    """

    def __init__(self, a_dim, s_dim, act_limits=None):
        """Initiates object state.

        Args:
            a_dim (int): Action space dimension.

            s_dim (int): Observation space dimension.

            act_limits (dict, optional): The "high" and "low" action bounds of the
                environment. Used for rescaling the actions that comes out of network
                from (-1, 1) to (low, high). Defaults to (-1, 1).
        """

        # Display information about the algorithm being used (LAC or SAC)
        if ALG_PARAMS["use_lyapunov"]:
            print(
                colorize("INFO: You are using the LAC algorithm.", "green", bold=True)
            )
        else:
            print(
                colorize("WARN: You are using the SAC algorithm.", "yellow", bold=True)
            )

        # Save action and observation space as members
        self._a_dim = a_dim
        self._s_dim = s_dim
        self._act_limits = act_limits

        # Save algorithm parameters as class objects
        self.use_lyapunov = ALG_PARAMS["use_lyapunov"]
        self._network_structure = ALG_PARAMS["network_structure"]
        self._polyak = 1 - ALG_PARAMS["tau"]
        self._gamma = ALG_PARAMS["gamma"]
        self._alpha_3 = ALG_PARAMS["alpha3"]

        # Determine target entropy
        # NOTE (rickstaa): If not defined we use the Lower bound of the policy entropy
        if ALG_PARAMS["target_entropy"] is None:
            self.target_entropy = -self._a_dim
        else:
            self.target_entropy = ALG_PARAMS["target_entropy"]

        # Create Learning rate placeholders
        self._lr_a = tf.Variable(ALG_PARAMS["lr_a"], name="LR_A")
        if self.use_lyapunov:
            self._lr_lag = tf.Variable(ALG_PARAMS["lr_a"], name="LR_lag")
            self._lr_l = tf.Variable(ALG_PARAMS["lr_l"], name="LR_L")
        else:
            self._lr_c = tf.Variable(ALG_PARAMS["lr_c"], name="LR_C")

        # Make sure alpha and alpha are not zero
        # NOTE (rickstaa): This is needed to prevent log_alpha/log_lambda from becoming
        # -np.inf
        ALG_PARAMS["alpha"] = (
            1e-37 if ALG_PARAMS["alpha"] == 0.0 else ALG_PARAMS["alpha"]
        )
        ALG_PARAMS["labda"] = (
            1e-37 if ALG_PARAMS["labda"] == 0.0 else ALG_PARAMS["labda"]
        )

        # Create placeholders for the Lagrance multipliers
        self.log_alpha = tf.Variable(tf.math.log(ALG_PARAMS["alpha"]), name="log_alpha")
        if self.use_lyapunov:
            self.log_labda = tf.Variable(
                tf.math.log(ALG_PARAMS["labda"]), name="log_lambda"
            )

        # Create Gaussian Actor (GA) and Lyapunov critic (LC) or Q-Critic (QC) networks
        self.ga = SquashedGaussianActor(
            obs_dim=self._s_dim,
            act_dim=self._a_dim,
            hidden_sizes=self._network_structure["actor"],
            act_limits=self.act_limits,
        )
        if self.use_lyapunov:
            self.lc = LyapunovCritic(
                obs_dim=self._s_dim,
                act_dim=self._a_dim,
                hidden_sizes=self._network_structure["critic"],
            )
        else:
            # NOTE (rickstaa): We create two Q-critics so we can use the Clipped
            # double-Q trick.
            self.q_1 = QCritic(
                obs_dim=self._s_dim,
                act_dim=self._a_dim,
                hidden_sizes=self._network_structure["q_critic"],
            )
            self.q_2 = QCritic(
                obs_dim=self._s_dim,
                act_dim=self._a_dim,
                hidden_sizes=self._network_structure["q_critic"],
            )

        # Create GA, LC and QC target networks
        # Don't get optimized but get updated according to the EMA of the main
        # networks
        self.ga_ = SquashedGaussianActor(
            obs_dim=self._s_dim,
            act_dim=self._a_dim,
            hidden_sizes=self._network_structure["actor"],
            act_limits=self.act_limits,
        )
        if self.use_lyapunov:
            self.lc_ = LyapunovCritic(
                obs_dim=self._s_dim,
                act_dim=self._a_dim,
                hidden_sizes=self._network_structure["critic"],
            )
        else:
            self.q_1_ = QCritic(
                obs_dim=self._s_dim,
                act_dim=self._a_dim,
                hidden_sizes=self._network_structure["q_critic"],
            )
            self.q_2_ = QCritic(
                obs_dim=self._s_dim,
                act_dim=self._a_dim,
                hidden_sizes=self._network_structure["q_critic"],
            )

        self._init_targets()

        # Create optimizers
        # NOTE (rickstaa): We here optimize for log_alpha and log_labda instead of
        # alpha and labda because it is more numerically stable (see:
        # https://github.com/rail-berkeley/softlearning/issues/136)
        self._alpha_train = tf.keras.optimizers.Adam(learning_rate=self._lr_a)
        self._a_train = tf.keras.optimizers.Adam(learning_rate=self._lr_a)
        if self.use_lyapunov:
            self._lambda_train = tf.keras.optimizers.Adam(learning_rate=self._lr_lag)
            self._l_train = tf.keras.optimizers.Adam(learning_rate=self._lr_l)
        else:
            self._main_q_vars = (
                self.q_1.trainable_variables + self.q_2.trainable_variables
            )  # Chain parameters of the two Q-critics
            self._q_train = tf.keras.optimizers.Adam(learning_rate=self._lr_c)

        # Create model save dict
        if self.use_lyapunov:
            self._save_dict = {
                "gaussian_actor": self.ga,
                "lyapunov_critic": self.lc,
                "log_alpha": self.log_alpha,
                "log_labda": self.log_labda,
                "use_lyapunov": self.use_lyapunov,
            }
        else:
            self._save_dict = {
                "gaussian_actor": self.ga,
                "q_critic_1": self.q_1,
                "q_critic_2": self.q_2,
                "log_alpha": self.log_alpha,
                "use_lyapunov": self.use_lyapunov,
            }

    @tf.function
    def choose_action(self, s, evaluation=False):
        """Returns the current action of the policy.

        Args:
            s (np.numpy): The current state.

            evaluation (bool, optional): Whether to return a deterministic action.
                Defaults to False.

        Returns:
            np.numpy: The current action.
        """

        # Make sure s is float32 tensorflow tensor
        if not isinstance(s, tf.Tensor):
            s = tf.convert_to_tensor(s, dtype=tf.float32)
        elif s.dtype != tf.float32:
            s = tf.cast(s, dtype=tf.float32)

        # Get current best action
        if evaluation is True:
            try:
                det_a, _ = self.ga(tf.reshape(s, (1, -1)), deterministic=True)
                return det_a[0]
            except ValueError:
                return
        else:
            a, _ = self.ga(tf.reshape(s, (1, -1)))
            return a[0]

    @tf.function
    def learn(self, lr_a, lr_l, lr_lag, lr_c, batch):
        """Runs the SGD to update all the optimize parameters.

        Args:
            lr_a (float): Current actor learning rate.

            lr_l (float): Lyapunov critic learning rate.

            lr_c (float): Q-Critic learning rate.

            lr_lag (float): Lyapunov constraint langrance multiplier learning rate.

            batch (numpy.ndarray): The batch of experiences.

        Returns:
            tuple: Tuple with some diagnostics about the training.
        """

        # Adjust optimizer learning rates (decay)
        self._set_learning_rates(
            lr_a=lr_a, lr_alpha=lr_a, lr_l=lr_l, lr_labda=lr_lag, lr_c=lr_c
        )

        ################################################
        # Optimize (Lyapunov/Q) critic #################
        ################################################
        if self.use_lyapunov:
            # Get target Lyapunov value (Bellman-backup)
            a2_, _ = self.ga_(
                batch["s_"]
            )  # NOTE (rickstaa): Target actions come from *current* *target* policy
            l_pi_targ = self.lc_(batch["s_"], a2_)
            l_backup = (
                batch["r"] + self._gamma * (1 - batch["terminal"]) * l_pi_targ
            )  # The Lyapunov candidate

            # Compute Lyapunov Critic error gradients
            with tf.GradientTape() as l_tape:

                # Get current Lyapunov value
                l1 = self.lc(batch["s"], batch["a"])

                # Calculate Lyapunov *CRITIC* error
                # NOTE (rickstaa): The 0.5 multiplication factor was added to make the
                # derivation cleaner and can be safely removed without influencing the
                # minimization. We kept it here for consistency.
                l_error = 0.5 * tf.reduce_mean((l1 - l_backup) ** 2)  # See eq. 7

            # Perform one gradient descent step for the Lyapunov critic
            l_grads = l_tape.gradient(l_error, self.lc.trainable_variables)
            self._l_train.apply_gradients(zip(l_grads, self.lc.trainable_variables))
        else:

            # Get target Q values (Bellman-backup)
            # NOTE (rickstaa): Here we use max-clipping instead of min-clipping used
            # in the SAC algorithm since we want to minimize the return.
            a2, logp_a2 = self.ga(
                batch["s_"]
            )  # NOTE (rickstaa): Target actions come from *current* policy
            q1_pi_targ = self.q_1_(batch["s_"], a2)
            q2_pi_targ = self.q_2_(batch["s_"], a2)
            q_pi_targ = tf.maximum(
                q1_pi_targ, q2_pi_targ
            )  # Use max clipping  to prevent overestimation bias.
            q_backup = batch["r"] + self._gamma * (1 - batch["terminal"]) * (
                q_pi_targ - self.alpha * logp_a2
            )

            # Compute the Q-Critic loss gradients
            with tf.GradientTape() as q_tape:
                # Get the current Q values
                q1 = self.q_1(batch["s"], batch["a"])
                q2 = self.q_2(batch["s"], batch["a"])

                # Calculate Q-critic loss
                loss_q1 = 0.5 * tf.reduce_mean(
                    (q1 - q_backup) ** 2
                )  # See Haarnoja eq. 5
                loss_q2 = 0.5 * tf.reduce_mean((q2 - q_backup) ** 2)
                loss_q = loss_q1 + loss_q2

            # Perform one gradient descent step for the Q-critic
            q_grads = q_tape.gradient(loss_q, self._main_q_vars)
            self._q_train.apply_gradients(zip(q_grads, self._main_q_vars))

        ################################################
        # Optimize Gaussian actor ######################
        ################################################

        # Compute actor loss gradients
        with tf.GradientTape() as a_tape:

            # Retrieve log probabilities of batch observations based on *current*
            # policy
            pi, log_pis = self.ga(batch["s"])

            # Compute actor loss
            if self.use_lyapunov:
                # Calculate the target Lyapunov value
                a2, _ = self.ga(
                    batch["s_"]
                )  # NOTE (rickstaa): Target actions come from *current* policy
                lya_l_ = self.lc(batch["s_"], a2)

                # Compute Lyapunov Actor error
                self.l_delta = tf.reduce_mean(
                    lya_l_ - tf.stop_gradient(l1) + self._alpha_3 * batch["r"]
                )  # See eq. 11

                # Calculate actor loss
                a_loss = tf.stop_gradient(self.labda) * self.l_delta + tf.stop_gradient(
                    self.alpha
                ) * tf.reduce_mean(
                    log_pis
                )  # See eq. 12
            else:

                # Retrieve the current Q values
                q1_pi = self.q_1(
                    batch["s"], pi
                )  # NOTE (rickstaa): Actions come from *current* policy
                q2_pi = self.q_2(
                    batch["s"], pi
                )  # NOTE (rickstaa): Actions come from *current* policy
                q_pi = tf.maximum(q1_pi, q2_pi)

                # Calculate actor loss
                a_loss = tf.reduce_mean(
                    tf.stop_gradient(self.alpha) * log_pis - q_pi
                )  # See Haarnoja eq. 7

        # Perform one gradient descent step for the Gaussian Actor
        a_grads = a_tape.gradient(a_loss, self.ga.trainable_variables)
        self._a_train.apply_gradients(zip(a_grads, self.ga.trainable_variables))

        ################################################
        # Optimize alpha (Entropy temperature) #########
        ################################################

        # Compute alpha loss gradients
        with tf.GradientTape() as alpha_tape:

            # Calculate alpha loss
            alpha_loss = -tf.reduce_mean(
                self.alpha * tf.stop_gradient(log_pis + self.target_entropy)
            )  # See Haarnoja eq. 17

        # Perform one gradient descent step for alpha
        alpha_grads = alpha_tape.gradient(alpha_loss, [self.log_alpha])
        self._alpha_train.apply_gradients(zip(alpha_grads, [self.log_alpha]))

        ################################################
        # Optimize labda (Lyapunov temperature) ########
        ################################################
        if self.use_lyapunov:

            # Compute labda loss gradients
            with tf.GradientTape() as lambda_tape:

                # Calculate labda loss
                # NOTE (rickstaa): Log_labda was used in the lambda_loss function
                # because using lambda caused the gradients to vanish. This is caused
                # since we restrict lambda within a 0-1.0 range using the clamp function
                # (see #38). Using log_lambda also is more numerically stable.
                labda_loss = -tf.reduce_mean(
                    self.log_labda * tf.stop_gradient(self.l_delta)
                )  # See formulas under eq. 14

            # Perform one gradient descent step for labda
            lambda_grads = lambda_tape.gradient(labda_loss, [self.log_labda])
            self._lambda_train.apply_gradients(zip(lambda_grads, [self.log_labda]))

        ################################################
        # Update target networks and return ############
        # diagnostics. #################################
        ################################################

        # Update target networks
        self._update_targets()

        # Return diagnostics
        if self.use_lyapunov:
            return (
                self.labda,
                self.alpha,
                l_error,
                tf.reduce_mean(tf.stop_gradient(-log_pis)),
                a_loss,
                alpha_loss,
                labda_loss,
            )
        else:
            return (
                self.alpha,
                loss_q,
                tf.reduce_mean(tf.stop_gradient(-log_pis)),
                a_loss,
                alpha_loss,
            )

    def save_result(self, path):
        """Saves current policy.

        Args:
            path (str): The path where you want to save the policy.
        """

        # Make save path absolute
        save_path = osp.abspath(osp.join(path, "policy"))

        # Create folder if it does not yet exist
        if not os.path.exists(save_path):
            os.makedirs(save_path)

        # Save all models/tensors in the _save_dict
        vars_dict = {}
        for name, item in self._save_dict.items():
            if issubclass(item.__class__, tf.keras.Model):
                item.save_weights(osp.join(save_path, name))
                print(
                    colorize(
                        f"Saved '{name}' weights to path: {save_path}",
                        "cyan",
                        bold=True,
                    )
                )
            elif issubclass(item.__class__, tf.Variable):
                vars_dict[name] = item.numpy()
            else:
                vars_dict[name] = item

        # Save vars dictionary
        with open(osp.join(save_path, "vars.json"), "w") as fp:
            vars_dict = convert_json(vars_dict)  # Convert to json format
            json_data = json.dumps(
                vars_dict, separators=(",", ":\t"), indent=4, sort_keys=True
            )
            fp.write(json_data)
            print(colorize("Saving other vars:\n", color="cyan", bold=True))
            print(colorize(json_data, "cyan", bold=True))

    def restore(self, path, restore_lagrance_multipliers=True):
        """Restores policy.

        Args:
            path (str): The path where you want to save the policy.

            restore_lagrance_multipliers (bool, optional): Whether you want to restore
                the lagrance multipliers.

        Returns:
            bool: Boolean specifying whether the policy was loaded successfully.
        """

        # Create load path
        load_path = osp.abspath(path)

        # Load train configuration
        try:
            with open(osp.join(load_path, "vars.json"), "r") as f:
                train_config = json.load(f)
        except (FileNotFoundError, NotADirectoryError):
            success_load = False
            return success_load

        # Throw warning if restored model is different from the model use now
        if self.use_lyapunov != train_config["use_lyapunov"]:
            alg_strings = [
                "LAC" if self.use_lyapunov else "SAC",
                "LAC" if train_config["use_lyapunov"] else "SAC",
            ]
            if TRAIN_PARAMS["continue_training"]:
                warn_str = colorize(
                    (
                        f"ERROR: You tried to load a {alg_strings[1]} model while the "
                        f"`variant.py` file specifies you want to train it as a"
                        f"{alg_strings[0]} model. Shutting down training as this is "
                        "not yet supported."
                    ),
                    "red",
                    bold=True,
                )
                print(warn_str)
                sys.exit(0)
            else:
                warn_str = colorize(
                    (
                        f"ERROR: You tried to load a {alg_strings[1]} model while the "
                        f"`variant.py` file specifies you want to use it in the "
                        f"inference as a {alg_strings[0]} model. As a result the "
                        "`variant.py` will be ignored."
                    ),
                    "yellow",
                    bold=True,
                )
                print(warn_str)
                self.__reload_critic_networks(use_lyapunov=train_config["use_lyapunov"])

        # Check if the models exist
        try:
            checkpoints = [
                f.replace(".index", "")
                for f in os.listdir(load_path)
                if f.endswith(".index")
            ]
        except (FileNotFoundError, NotADirectoryError):
            success_load = False
            return success_load

        # Check if any checkpoints were found
        if not checkpoints:
            success_load = False
            return success_load

        # Restore network parameters
        try:
            if train_config["use_lyapunov"]:
                self.ga.load_weights(load_path + "/gaussian_actor")
                self.lc.load_weights(load_path + "/lyapunov_critic")
                if restore_lagrance_multipliers:
                    self.log_alpha = train_config["log_alpha"]
                    self.log_labda = train_config["log_labda"]
            else:
                self.ga.load_weights(load_path + "/gaussian_actor")
                self.q_1.load_weights(load_path + "/q_critic_1")
                self.q_2.load_weights(load_path + "/q_critic_2")
                if restore_lagrance_multipliers:
                    self.log_alpha = train_config["log_alpha"]
        except (KeyError, AttributeError):
            alg_string = "LAC" if train_config["use_lyapunov"] else "SAC"
            print(
                colorize(
                    (
                        "ERROR: Something went wrong while trying to load the "
                        f"{alg_string} model. Shutting down the training."
                    ),
                    "red",
                    bold=True,
                )
            )
            sys.exit(0)

        # Return result
        success_load = True
        return success_load

    def __reload_critic_networks(self, use_lyapunov):
        """Function used to reload the right networks when the loaded model type
        differs from the type set in the `variant.py` file. Currently only used during
        inference.

        Args:
            use_lyapunov (bool): Whether the new setup should use lyapunov or not.
        """
        # Create required networks
        if use_lyapunov:  # LAC

            # Print reload message
            print(
                colorize(
                    "INFO: You switched to using the LAC algorithm.",
                    "green",
                    bold=True,
                )
            )

            # Create log_labda
            self.log_labda = tf.Variable(
                tf.math.log(ALG_PARAMS["labda"]), name="log_lambda"
            )

            # Create main and target Lyapunov Critic networks
            self.lc = LyapunovCritic(
                obs_dim=self._s_dim,
                act_dim=self._a_dim,
                hidden_sizes=self._network_structure["critic"],
            )
            self.lc_ = LyapunovCritic(
                obs_dim=self._s_dim,
                act_dim=self._a_dim,
                hidden_sizes=self._network_structure["critic"],
            )

            # Remove main and target Q-Critic networks
            # NOTE (rickstaa): Removed to make sure we notice if something goes wrong.
            delattr(self, "q_1")
            delattr(self, "q_2")
            delattr(self, "q_1_")
            delattr(self, "q_2_")
        else:  # SAC

            # Print reload message
            print(
                colorize(
                    "WARN: You switched to using the SAC algorithm.",
                    "yellow",
                    bold=True,
                )
            )

            # Create main and target Q-Critic networks
            self.q_1 = QCritic(
                obs_dim=self._s_dim,
                act_dim=self._a_dim,
                hidden_sizes=self._network_structure["q_critic"],
            )
            self.q_2 = QCritic(
                obs_dim=self._s_dim,
                act_dim=self._a_dim,
                hidden_sizes=self._network_structure["q_critic"],
            )
            self.q_1_ = QCritic(
                obs_dim=self._s_dim,
                act_dim=self._a_dim,
                hidden_sizes=self._network_structure["q_critic"],
            )
            self.q_2_ = QCritic(
                obs_dim=self._s_dim,
                act_dim=self._a_dim,
                hidden_sizes=self._network_structure["q_critic"],
            )

            # Remove main and target Q-Critic networks
            delattr(self, "lc")
            delattr(self, "lc_")

    def _set_learning_rates(
        self, lr_a=None, lr_alpha=None, lr_l=None, lr_labda=None, lr_c=None
    ):
        """Adjusts the learning rates of the optimizers.

        Args:
            lr_a (float, optional): The learning rate of the actor optimizer. Defaults
                to None.

            lr_alpha (float, optional): The learning rate of the temperature optimizer.
                Defaults to None.

            lr_l (float, optional): The learning rate of the Lyapunov critic. Defaults
                to None.

            lr_labda (float, optional): The learning rate of the Lyapunov Lagrance
                multiplier optimizer. Defaults to None.

            lr_c (float, optional): The learning rate of the Q-Critic optimizer.
                Defaults to None.
        """
        if lr_a:
            self._alpha_train.lr.assign(lr_a)
        if lr_alpha:
            self._a_train.lr.assign(lr_alpha)
        if self.use_lyapunov:
            if lr_l:
                self._l_train.lr.assign(lr_l)
            if lr_labda:
                self._lambda_train.lr.assign(lr_labda)
        else:
            if lr_c:
                self._q_train.lr.assign(lr_c)

    @tf.function
    def _init_targets(self):
        """Updates the target network weights to the main network weights.
        """
        for ga_main, ga_targ in zip(self.ga.variables, self.ga_.variables):
            ga_targ.assign(ga_main)
        if self.use_lyapunov:
            for lc_main, lc_targ in zip(self.lc.variables, self.lc_.variables):
                lc_targ.assign(lc_main)
        else:
            for q_1_main, q_1_targ in zip(self.q_1.variables, self.q_1_.variables):
                q_1_targ.assign(q_1_main)
            for q_2_main, q_2_targ in zip(self.q_2.variables, self.q_2_.variables):
                q_2_targ.assign(q_2_main)

    @tf.function
    def _update_targets(self):
        """Updates the target networks based on a Exponential moving average
        (Polyak averaging).
        """
        for ga_main, ga_targ in zip(self.ga.variables, self.ga_.variables):
            ga_targ.assign(self._polyak * ga_targ + (1 - self._polyak) * ga_main)
        if self.use_lyapunov:
            for lc_main, lc_targ in zip(self.lc.variables, self.lc_.variables):
                lc_targ.assign(self._polyak * lc_targ + (1 - self._polyak) * lc_main)
        else:
            for q_1_main, q_1_targ in zip(self.q_1.variables, self.q_1_.variables):
                q_1_targ.assign(self._polyak * q_1_targ + (1 - self._polyak) * q_1_main)
            for q_2_main, q_2_targ in zip(self.q_2.variables, self.q_2_.variables):
                q_2_targ.assign(self._polyak * q_2_targ + (1 - self._polyak) * q_2_main)

    @property
    def alpha(self):
        """Property used to clip alpha to be equal or bigger than 0.0 to prevent it from
        becoming nan when log_alpha becomes -inf. For alpha no upper bound is used.
        """
        return tf.clip_by_value(tf.exp(self.log_alpha), *SCALE_ALPHA_MIN_MAX)

    @property
    def labda(self):
        """Property used to clip lambda to be equal or bigger than 0.0 in order to
        prevent it from becoming nan when log_labda becomes -inf. Further we clip it to
        be lower or equal than 1.0 in order to prevent lambda from exploding when the
        the hyperparameters are chosen badly.
        """
        return tf.clip_by_value(tf.exp(self.log_labda), *SCALE_LAMBDA_MIN_MAX)

    @property
    def act_limits(self):
        return self._act_limits

    @act_limits.setter
    def act_limits(self, act_limits):
        """Sets the action limits that are used for scaling the actions that are
        returned from the gaussian policy.
        """

        # Validate input
        missing_keys = [key for key in ["low", "high"] if key not in act_limits.keys()]
        if missing_keys:
            warn_string = "WARN: act_limits could not be set as {} not found.".format(
                f"keys {missing_keys} were"
                if len(missing_keys) > 1
                else f"key {missing_keys} was"
            )
            print(colorize(warn_string, "yellow"))
        invalid_length = [
            key for key, val in act_limits.items() if len(val) != self._a_dim
        ]
        if invalid_length:
            warn_string = (
                f"WARN: act_limits could not be set as the length of {invalid_length} "
                + "{}".format("were" if len(invalid_length) > 1 else "was")
                + f" unequal to the dimension of the action space (dim={self._a_dim})."
            )
            print(colorize(warn_string, "yellow"))

        # Set action limits
        self._act_limits = {"low": act_limits["low"], "high": act_limits["high"]}
        self.ga.act_limits = self._act_limits


def train(log_dir):
    """Performs the agent training.

    Args:
        log_dir (str): The directory in which the final model (policy) and the log data
            is saved.
    """

    # Create train and test environments
    print(
        colorize(
            f"INFO: You are training in the {ENV_NAME} environment.", "cyan", bold=True,
        )
    )
    env = get_env_from_name(ENV_NAME, ENV_SEED)
    test_env = get_env_from_name(ENV_NAME, ENV_SEED)

    # Set initial learning rates
    lr_a, lr_l, lr_c = (
        ALG_PARAMS["lr_a"],
        ALG_PARAMS["lr_l"],
        ALG_PARAMS["lr_c"],
    )
    lr_a_now = ALG_PARAMS["lr_a"]  # learning rate for actor, lambda and alpha
    lr_l_now = ALG_PARAMS["lr_l"]  # learning rate for Lyapunov critic
    lr_c_now = ALG_PARAMS["lr_c"]  # learning rate for q critic

    # Get observation and action space dimension and limits from the environment
    s_dim = env.observation_space.shape[0]
    a_dim = env.action_space.shape[0]
    a_lowerbound = env.action_space.low
    a_upperbound = env.action_space.high

    # Create the Agent
    policy = LAC(a_dim, s_dim, act_limits={"low": a_lowerbound, "high": a_upperbound})

    # Load model if retraining is selected
    if TRAIN_PARAMS["continue_training"]:

        # Create retrain model path
        retrain_model_folder = TRAIN_PARAMS["continue_model_folder"]
        retrain_model_path = osp.abspath(
            osp.join(log_dir, "../..", TRAIN_PARAMS["continue_model_folder"])
        )

        # Check if retrain model exists if not throw error
        if not osp.exists(retrain_model_path):
            print(
                colorize(
                    (
                        "ERROR: Shutting down training since the model you specified "
                        f"in the `continue_model_folder` `{retrain_model_folder}` "
                        f"argument was not found for the `{ENV_NAME}` environment."
                    ),
                    "red",
                    bold=True,
                )
            )
            sys.exit(0)

        # Load old model
        print(
            colorize(
                f"INFO: Restoring model `{retrain_model_path}`.", "cyan", bold=True
            )
        )
        result = policy.restore(
            osp.abspath(osp.join(retrain_model_path, "policy")),
            restore_lagrance_multipliers=(not ALG_PARAMS["reset_lagrance_multipliers"]),
        )
        if not result:
            print(
                colorize(
                    "ERROR: Shuting down training as something went wrong while "
                    "loading "
                    f"model `{retrain_model_folder}`.",
                    "red",
                    bold=True,
                )
            )
            sys.exit(0)

        # Create new storage folder
        log_dir_split = log_dir.split("/")
        log_dir_split[-2] = (
            "_".join(TRAIN_PARAMS["continue_model_folder"].split("/")) + "_finetune"
        )
        log_dir = "/".join(log_dir_split)
    else:
        print(colorize(f"INFO: Train new model `{log_dir}`", "cyan", bold=True))

    # Print logging folder path
    print(colorize(f"INFO: Logging results to `{log_dir}`.", "cyan", bold=True))

    # Create replay memory buffer
    pool = Pool(
        s_dim=s_dim,
        a_dim=a_dim,
        store_last_n_paths=TRAIN_PARAMS["num_of_training_paths"],
        memory_capacity=ALG_PARAMS["memory_capacity"],
        min_memory_size=ALG_PARAMS["min_memory_size"],
    )

    # Setup logger and log hyperparameters
    logger.configure(dir=log_dir, format_strs=["csv"])
    logger.logkv("tau", ALG_PARAMS["tau"])
    logger.logkv("alpha3", ALG_PARAMS["alpha3"])
    logger.logkv("batch_size", ALG_PARAMS["batch_size"])
    logger.logkv("target_entropy", policy.target_entropy)

    ####################################################
    # Training loop ####################################
    ####################################################

    # Setup training loop parameters
    t1 = time.time()
    global_step = 0
    global_episodes = 0
    last_training_paths = deque(maxlen=TRAIN_PARAMS["num_of_training_paths"])
    training_started = False

    # Train the agent in the environment until max_episodes has been reached
    print(colorize("INFO: Training...\n", "cyan", bold=True))
    while 1:  # Keep running episodes until global step has been reached

        # Create variable to store information about the current path
        if policy.use_lyapunov:
            current_path = {
                "rewards": [],
                "lyapunov_error": [],
                "alpha": [],
                "lambda": [],
                "entropy": [],
                "a_loss": [],
                "alpha_loss": [],
                "lambda_loss": [],
            }
        else:
            current_path = {
                "rewards": [],
                "critic_error": [],
                "alpha": [],
                "entropy": [],
                "a_loss": [],
                "alpha_loss": [],
            }

        # Break out of loop if global steps have been reached
        if global_step > TRAIN_PARAMS["max_global_steps"]:

            # Print step count, save model and stop the program
            print(
                colorize(
                    f"\nINFO: Training stopped after {global_step} steps.",
                    "cyan",
                    bold=True,
                )
            )
            print(
                colorize(
                    "INFO: Running time: {}".format(time.time() - t1),
                    "cyan",
                    bold=True,
                )
            )
            print(colorize("INFO: Saving Model", "cyan", bold=True))
            policy.save_result(log_dir)
            return

        # Reset environment
        s = env.reset()

        # Training Episode loop
        for jj in range(ENVS_PARAMS[ENV_NAME]["max_ep_steps"]):

            # Save intermediate checkpoints if requested
            if TRAIN_PARAMS["save_checkpoints"]:
                if (
                    global_step % TRAIN_PARAMS["checkpoint_save_freq"] == 0
                    and global_step != 0
                ):

                    # Create intermediate result checkpoint folder
                    checkpoint_save_path = osp.abspath(
                        osp.join(log_dir, "checkpoints", "step_" + str(jj))
                    )
                    os.makedirs(checkpoint_save_path, exist_ok=True)

                    # Save intermediate checkpoint
                    policy.save_result(checkpoint_save_path)

            # Render environment if requested
            if ENVS_PARAMS[ENV_NAME]["eval_render"]:
                env.render()

            # Retrieve (scaled) action based on the current policy
            # NOTE (rickstaa): The scaling operation is already performed inside the
            # policy based on the `act_limits` you supplied.
            a = policy.choose_action(s)

            # Perform action in env
            s_, r, done, _ = env.step(a)

            # Increment global step count
            if training_started:
                global_step += 1

            # Stop episode if max_steps has been reached
            if jj == ENVS_PARAMS[ENV_NAME]["max_ep_steps"] - 1:
                done = True
            terminal = 1.0 if done else 0.0

            # Store experience in replay buffer
            pool.store(s, a, r, terminal, s_)

            # Optimize network weights and lagrance multipliers
            if (
                pool.memory_pointer > ALG_PARAMS["min_memory_size"]
                and global_step % ALG_PARAMS["steps_per_cycle"] == 0
            ):
                training_started = True

                # Perform STG a set number of times (train per cycle)
                for _ in range(ALG_PARAMS["train_per_cycle"]):
                    batch = pool.sample(ALG_PARAMS["batch_size"])
                    if policy.use_lyapunov:
                        (
                            labda,
                            alpha,
                            l_loss,
                            entropy,
                            a_loss,
                            alpha_loss,
                            labda_loss,
                        ) = policy.learn(lr_a_now, lr_l_now, lr_a, lr_c_now, batch)
                    else:
                        alpha, loss_q, entropy, a_loss, alpha_loss = policy.learn(
                            lr_a_now, lr_l_now, lr_a, lr_c_now, batch
                        )

            # Store current path results
            if training_started:
                if policy.use_lyapunov:
                    current_path["rewards"].append(r)
                    current_path["lyapunov_error"].append(l_loss)
                    current_path["alpha"].append(alpha)
                    current_path["lambda"].append(labda)
                    current_path["entropy"].append(entropy)
                    current_path["a_loss"].append(a_loss)
                    current_path["alpha_loss"].append(alpha_loss)
                    current_path["lambda_loss"].append(labda_loss)
                else:
                    current_path["rewards"].append(r)
                    current_path["critic_error"].append(loss_q.numpy())
                    current_path["alpha"].append(alpha.numpy())
                    current_path["entropy"].append(entropy.numpy())
                    current_path["a_loss"].append(
                        a_loss.numpy()
                    )  # Improve: Check if this is the fastest way
                    current_path["alpha_loss"].append(alpha_loss)

            # Evalute the current policy performance and log the results
            if (
                training_started
                and global_step % TRAIN_PARAMS["evaluation_frequency"] == 0
                and global_step > 0
            ):
                logger.logkv("total_timesteps", global_step)
                training_diagnostics = evaluate_training_rollouts(last_training_paths)
                if training_diagnostics is not None:
                    if TRAIN_PARAMS["num_of_evaluation_paths"] > 0:
                        eval_diagnostics = training_evaluation(test_env, policy)
                        [
                            logger.logkv(key, eval_diagnostics[key])
                            for key in eval_diagnostics.keys()
                        ]
                        training_diagnostics.pop("return")
                    [
                        logger.logkv(key, training_diagnostics[key])
                        for key in training_diagnostics.keys()
                    ]
                    logger.logkv("lr_a", lr_a_now)
                    if policy.use_lyapunov:
                        logger.logkv("lr_l", lr_l_now)
                    else:
                        logger.logkv("lr_c", lr_c_now)
                    string_to_print = ["time_step:", str(global_step), "|"]
                    if TRAIN_PARAMS["num_of_evaluation_paths"] > 0:
                        [
                            string_to_print.extend(
                                [key, ":", str(eval_diagnostics[key]), "|"]
                            )
                            for key in eval_diagnostics.keys()
                        ]
                    [
                        string_to_print.extend(
                            [key, ":", str(round(training_diagnostics[key], 2)), "|"]
                        )
                        for key in training_diagnostics.keys()
                    ]  # Improve: Check if this is the fastest way
                    # [
                    #     string_to_print.extend(
                    #         [
                    #             key,
                    #             ":",
                    #             str(
                    #                 (training_diagnostics["length"] * 10 ** 2)
                    #                 .round()
                    #                 .numpy()
                    #                 / (10 ** 2)
                    #             ),
                    #             "|",
                    #         ]
                    #     )
                    #     for key in training_diagnostics.keys()
                    # ]
                    prefix = (
                        colorize("LAC|", "green")
                        if ALG_PARAMS["use_lyapunov"]
                        else colorize("SAC|", "yellow")
                    )
                    print(
                        colorize(prefix, "yellow", bold=True) + "".join(string_to_print)
                    )
                logger.dumpkvs()

            # Update state
            s = s_

            # Check if episode is done (continue to next episode)
            if done:

                # Store paths
                if training_started:
                    last_training_paths.appendleft(current_path)

                # Decay learning rates
                frac = 1.0 - (global_step - 1.0) / TRAIN_PARAMS["max_global_steps"]
                lr_a_now = lr_a * frac  # learning rate for actor, lambda, alpha
                lr_l_now = lr_l * frac  # learning rate for Lyapunov critic
                lr_c_now = lr_c * frac  # learning rate for q critic
                break  # Continue to next episode

    # Increase episode counter
    global_episodes += 1
